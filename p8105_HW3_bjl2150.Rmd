---
title: "p8105_HW3_bjl2150"
author: "Briana Lettsome"
date: "October 15, 2018"
output: github_document
---

```{r}
# Loading of tidyverse package
library(tidyverse)
```

# Problem 1

```{r}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")


library(p8105.datasets)


data(brfss_smart2010)
  
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health")

# Checked the unique observations within 'response' columns
unique(brfss$response)

# Ordering of the responses from "Excellent"" to "Poor"
brfss %>%
  mutate(response_levels = forcats::fct_relevel(response, c("Excellent", 
                                                            "Very good", "Good",
                                                            "Fair", "Poor")))
```

### Question 1.1
```{r}
#Wrangling dataframe to find the which states were observed at 7 locations
brfss_2002 = brfss %>%
  filter(year == 2002) %>%
  separate(locationdesc, into = c("state", "county"), sep = "-") %>%
  group_by(state) %>%
  distinct(county) %>%
  count() %>%
  filter(n == 7)
```

Conecticut, Florida and North Carolina are the states which were observed at 
7 locations  

  
## Question 1.2: Making of Spaghetti Plot


```{r}
library(ggridges)

# Wanted to see the unique year observations within the dataset
unique(brfss_smart2010$Year)

# Making of the spaghetti plot
brfss_spaghetti = brfss %>%
  group_by(locationabbr, year) %>%
  distinct(locationdesc) %>%
  summarize(location_numbers = n()) %>%
  ggplot(aes(x = year, y = location_numbers, color = locationabbr)) + 
  geom_line() 

# Calling of the datset in order to show the plot.
brfss_spaghetti

```
In the above code chunk, library(ggridges) was loaded in order for plot to be 
properly displayed. The unique function was used to identify the unique years 
that are included wihtin the dataset.
In making the spaghetti plot, data was first grouped by locationabbr and year 
columns. The distinct function was then used on locationdesc to get the 
specific locations within each state. The summarize was used to obtain the 
numbers of locations within esch state. A spaghetti plot was then used to 
show the number of locations in each state from 2002 through 2010.

### Question 1.3: Making of Table
```{r}
# Using filter and spread functions to obtain the proportion of "Excellent" responses
brfss_table_1 = brfss %>%
    filter(locationabbr == "NY", response == "Excellent") %>%
  spread(key = response, value = data_value) %>%
  filter(year %in% c("2002", "2006", "2010")) 

# Making of the table to show means and standard deviations
brfss_table_1 %>%
  group_by(year) %>%
  summarize(mean_excellent = mean(Excellent),
            sd_excellent = sd(Excellent)) %>%
   knitr::kable(digits = 1)
 
```

### Question 4

```{r}
brfss_4 = brfss %>%
  mutate(response_levels = forcats::fct_relevel(response, c("Excellent",  
                                                            "Very good", "Good", 
                                                            "Fair", "Poor"))) %>%
  select(year, locationabbr, data_value, response_levels) %>%
  group_by(response_levels, locationabbr, year) %>%
  summarize(avg_response = mean(data_value))

brfss_4 %>%
    ggplot(aes(x = year, y = avg_response, group = year, color = response_levels)) +
  geom_boxplot() + 
  facet_grid(~response_levels) + 
  viridis::scale_fill_viridis(discrete = TRUE) %>%
   labs(
    title = "Average Proportion of Each Response Category",
    x = "Year",
    y = "Average Proportion",
    caption = "Data from brfss package"
  )

```

In the following code chunk, the brfss_4 dataframe was created from the modified
brfss dataset. The response levels were releveled into a new variable 
'response_levels which was then specifically selected along with the columns 'year', 
'locationabbr', and 'data_value'. The average response was caluclated taking the 
mean of 'data_value'. 

A separate code was used to create the 5-panel boxplot for all response_levels 
for each year and state. Additonally, title name, labelled X- and Y- axes, and 
caption were included in the plot.

# Problem 2
## Instacart dataset

```{r}
# Importing instacart dataset from p8105 datasets on github
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)

data(instacart)

# Calling 'instacart' data into my environment and cleaning names

instacart = instacart %>% 
  janitor::clean_names()

# Used dim function to get the size of the dataset
dim(instacart)
```

### Code chunk summary

The instacart dataset was imported into R. The dataset was then read into Rstudio 
and called so that it was recognized by the golabl environment. The 'instacart'
data was then cleaned using the janitor function. 

### Short Description of dataset

This dataset is from the delivery service Instacart. It shows key observations 
ranging from proudct ID number and the hour of the day that a product was
ordered to the product name and what aisle and department said product can be 
found in. Using the 'dim' function, this dataset has 15 columns and 1,384,617 rows.
This dataset is structured in such a way that would be unfamilar to the general
public. For instance, the variable 'order_dow" is very unclear and would need to 
be modified so that readers are aware that it refers to the order day of the week.
In whole, this dataset would need to be restructured to that it is easy to read.


## Question 2.1
```{r}
library(tidyverse)

# Answering how many aisles there are
instacart_1A = instacart %>%
  select(aisle) %>%
  distinct() %>%
  count()

# Answering which aisles are the most items ordered from
instacart_1B = instacart %>%
  select(product_name, aisle) %>% 
  count(aisle) %>% 
  arrange(desc(n))
```
In the first part of Question 1, the number of aisles was determined by selecting
the aisle column specifically, and then using the distinct function to determine 
the distinct aisles present in the dataset. The count function was then used to 
get a number of distinct aisles present in the dataset.

In the second part of the question, the columns product_name and aisle were
specfically selected. The count function was then used on the aisle column in 
order to get how many products are ordered per aisle. Then the arranged fucntion
was utilized to get which aisles had the largest amount of products order from 
them in descending order.

## Problem 2.2 - Making of plot
```{r}
instacart_2 = instacart %>%
  select(aisle, product_name) %>%
  group_by(aisle) %>%
  summarize(number_of_items = n()) %>%
  ggplot(aes(x = aisle, y = number_of_items)) + 
  geom_point(aes(color = aisle), alpha = .5) + 
  labs(
    title = "Temperature plot",
    x = "Minimum daily temperature (C)",
    y = "Maxiumum daily temperature (C)",
    caption = "Data from the rnoaa package"
  ) +
   theme(legend.position = "bottom")

instacart_2
```

## Problem 2.3 - Making table of most popular items

```{r}
# Structured the code to obain the correct output
instacart_3B = instacart %>%
  select(aisle, product_name) %>% 
  group_by(aisle) %>%
  count(product_name) %>%
  filter(aisle %in% c("baking ingredients", "dog food care", 
                      "packaged vegetables fruits")) %>% 
  arrange(desc(n), .by_group = TRUE ) %>%
  filter(product_name %in% c("Light Brown Sugar", "Snack Sticks Chicken & Rice Recipe Dog Treats",
                             "Organic Baby Spinach")) %>%
    knitr::kable(digits = 1)
 
# Called the dataset to be into the global environment
instacart_3B
```

To answer Question 2.3, a new dataset, named instacart_3B, was created from the 
original instacart dataset. The select function was used to specifically select
the columns aisle and product_name. The dataset was then grouped by aisle and 
the observations under the product_name counted.The three specific aisles were 
filtered out :'baking ingredients', 'dog food care', and 
'packaged vegetables fruits'. The dataset was arranged in descending orer by n 
and by group. The most popular items per aisle were then filtered out and a table
was created.
  

## Problem 4 - Making table of mean hour of day for Pink Lady Apples and Coffee Ice Cream
```{r}
# Manipulating of the dataset to be able to make the table
instacart_4 = instacart %>%
    select(order_hour_of_day, product_name, order_dow) %>%
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
    group_by(order_dow, product_name) %>%
    summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>%
    spread(key = order_dow, value = mean_hour) %>% 
    knitr::kable(digits = 1)
```
To determine the mean hour of the day that Pink Lady Apples and Coffee Ice Cream 
ordered, the select funtion was used to specifically select columns 
"order_hour_of_day", "product_name", and "order_dow". The filter function was 
then used to specify the observations for Pink Lady Apples and Coffee Ice Cream.
The dataset was then grouped by "order_dow" and "product_name" and then the 
"mean_hour" caluclated by takin the mean of "order_hout_of_day", while removing 
missing variables. The spread function was used to spread the "order_dow" from 
one column to multiple columns to reflect the days of the week. The value for 
this column was the previously calculated mean_hour values. knits::kable was
then used to create a table out of this dataset.

## Problem 2 Answers

* There are 134 aisles in the instacart dataset.

* The aisle with the most items ordered are fresh vegetables, fresh fruits, and
packaged vegetables fruits.

* A table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice 
Cream are ordered each day:
`r instacart_4`

# Problem 3
```{r}
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)

data(ny_noaa)

ny_noaa = ny_noaa
```

### Short Description of dataset
This dataset is provided by the National Oceanic and Atmosperic Administration. 
This particular set includes information from the various weather stations 
throughout NY state. Some of the the variables included within this dataset are 
weather station ID, precipiation (mm), snow depth (mm), and minimum and maximum 
temperatures. One of the primary issues with this dataset is the large amount of
missing data. Thus, it is difficlt to obtain an accurate measure of the variables,
such as precipiation or snowfall (mm), from the various weather stations.

## Problem 3.1
```{r}
# Cleaning and separating 'date' into 'year', 'month', and 'day'. 
# Then selected "snow"

ny_noaa_1 = ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  select(snow)

# Used the sort function in order to determine the most commonly observed valuy  
  sort(table(ny_noaa_1$snow), decreasing = TRUE) %>% View()
```

The most commonly observed value is 0 mm because, in the state of NY, it does not 
snow for more than half of the year (i.e. it snows for 4 to 5 months of the year).
Based on this, 0 mm of snow should be the most frequent observations. The second
and third commonly observed values are 25 and 13 mm. 

## Problem 3.2
```{r}
ny_noaa_2 = ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  select(year, month, tmax, id) %>%
  group_by(year, id, tmax)


```