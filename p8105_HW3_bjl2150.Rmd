---
title: "p8105_HW3_bjl2150"
author: "Briana Lettsome"
date: "October 15, 2018"
output: github_document
---

```{r}
# Loading of tidyverse package
library(tidyverse)
```

# Problem 1

```{r}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")


library(p8105.datasets)


data(brfss_smart2010)
  
brfss = brfss_smart2010 %>% 
  janitor::clean_names() %>%
  filter(topic == "Overall Health")

# Checked the unique observations within 'response' columns
unique(brfss$response)

# Ordering of the responses from "Excellent"" to "Poor"
brfss %>%
  mutate(response_levels = forcats::fct_relevel(response, c("Excellent", 
                                                            "Very good", "Good",
                                                            "Fair", "Poor")))
```

### Question 1.1
```{r}
#Wrangling dataframe to find the which states were observed at 7 locations
brfss_2002 = brfss %>%
  filter(year == 2002) %>%
  separate(locationdesc, into = c("state", "county"), sep = "-") %>%
  group_by(state) %>%
  distinct(county) %>%
  count() %>%
  filter(n == 7)
```

Conecticut, Florida and North Carolina are the states which were observed at 
7 locations  

  
## Question 1.2: Making of Spaghetti Plot


```{r}
library(ggridges)

# Wanted to see the unique year observations within the dataset
unique(brfss_smart2010$Year)

# Making of the spaghetti plot
brfss_spaghetti = brfss %>%
  group_by(locationabbr, year) %>%
  distinct(locationdesc) %>%
  summarize(location_numbers = n()) %>%
  ggplot(aes(x = year, y = location_numbers, color = locationabbr)) + 
  geom_line() 

# Calling of the datset in order to show the plot.
brfss_spaghetti

```
In the above code chunk, library(ggridges) was loaded in order for plot to be 
properly displayed. The unique function was used to identify the unique years 
that are included wihtin the dataset.
In making the spaghetti plot, data was first grouped by locationabbr and year 
columns. The distinct function was then used on locationdesc to get the 
specific locations within each state. The summarize was used to obtain the 
numbers of locations within esch state. A spaghetti plot was then used to 
show the number of locations in each state from 2002 through 2010.

### Question 1.3: Making of Table
```{r}
# Using filter and spread functions to obtain the proportion of "Excellent" responses
brfss_table_1 = brfss %>%
    filter(locationabbr == "NY", response == "Excellent") %>%
  spread(key = response, value = data_value) %>%
  filter(year %in% c("2002", "2006", "2010")) 

# Making of the table to show means and standard deveiations
brfss_table_1 %>%
  group_by(year) %>%
  summarize(mean_excellent = mean(Excellent),
            sd_excellent = sd(Excellent)) %>%
   knitr::kable(digits = 1)
 
```

### Question 4

```{r}
brfss_4 = brfss %>%
  mutate(response_levels = forcats::fct_relevel(response, c("Excellent",  
                                                            "Very good", "Good", 
                                                            "Fair", "Poor"))) %>%
  select(year, locationabbr, data_value, response_levels) %>%
  group_by(response_levels, locationabbr, year) %>%
  summarize(avg_response = mean(data_value))

brfss_4 %>%
    ggplot(aes(x = year, y = avg_response, group = year, color = response_levels)) +
  geom_boxplot() + 
  facet_grid(~response_levels) + 
  viridis::scale_fill_viridis(discrete = TRUE) %>%
   labs(
    title = "Average Proportion of Each Response Category",
    x = "Year",
    y = "Average Proportion",
    caption = "Data from brfss package"
  )

```

In the following code chunk, the brfss_4 dataframe was created from the modified
brfss dataset. The response levels were releveled into a new variable 
'response_levels which was then specifically selected along with the columns 'year', 
'locationabbr', and 'data_value'. The average response was caluclated taking the 
mean of 'data_value'. 

A separate code was used to create the 5-panel boxplot for all response_levels 
for each year and state. Additonally, title name, labelled X- and Y- axes, and 
caption were included in the plot.

# Problem 2
## Instacart dataset

```{r}
# Importing instacart dataset from p8105 datasets on github
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)

data(instacart)

# Calling 'instacart' data into my environment and cleaning names

instacart = instacart %>% 
  janitor::clean_names()

# Used dim function to get the size of the dataset
dim(instacart)
```

### Code chunk summary

The instacart dataset was imported into R. The dataset was then read into Rstudio 
and called so that it was recognized by the golabl environment. The 'instacart'
data was then cleaned using the janitor function. 

### Short Description of dataset

This dataset is from the delivery service Instacart. It shows observations ranging
from proudct ID number and the hour of the day that a product was ordered to the 
product name and what aisle and department said product can be found in.

Using the dim function, this dataset has 15 columns and 1,384,617 rows.


## Question 2.1
```{r}
library(tidyverse)

# Answering how many aisles there are
instacart_1A = instacart %>%
  select(aisle) %>%
  distinct() %>%
  count()

# Answering which aisles are the most items ordered from
instacart_1B = instacart %>%
  select(product_name, aisle) %>% 
  count(aisle) %>% 
  arrange(desc(n))

# Problem 2 - Making of plot

# Problem 3 - Making table of most popular items

instacart_3B = instacart %>%
  select(aisle, product_name) %>% 
  group_by(aisle) %>%
  count(product_name) %>%
  arrange(desc(n)) %>%
  filter(aisle %in% c("baking ingredients", "dog food care", 
                      "packaged vegetables fruits"))  

  

  
# Problem 4 - Making table of mean hour of day for Pink Lady and Coffee Ice Cream

instacart_4 = instacart %>%
    select(order_hour_of_day, product_name, order_dow) %>%
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
    group_by(order_dow, product_name) %>%
    summarize(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>%
    spread(key = order_dow, value = mean_hour) %>% 
    knitr::kable(digits = 1)
```

There are 134 aisles in the instacart dataset.

The aisle with the most items ordered are fresh vegetables, fresh fruits, and
packages vegetables fruits.

A table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice 
Cream are ordered each day:
`r instacart_4`

# Problem 3
```{r}
devtools::install_github("p8105/p8105.datasets")

library(p8105.datasets)

data(ny_noaa)

ny_noaa = ny_noaa
```

### Short Description of dataset
This dataset is provided by the National Oceanic and Atmosperic Administration. 
This particular set includes information from the various weather stations 
throughout NY state. Some of the the variables included within this dataset are 
weather station ID, precipiation (mm), snow depth (mm), and minimum and maximum 
temperatures. One of the primary issues with this dataset is the large amount of
missing data. Thus, it is difficlt to obtain an accurate measure of the variables,
such as precipiation or snowfall (mm), from the various weather stations.

## Problem 3.1
```{r}
# Cleaning and separating 'date' into 'year', 'month', and 'day'. 
# Then selected "snow"

ny_noaa_1 = ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  select(snow)

# Used the sort function in order to determine the most commonly observed valuy  
  sort(table(ny_noaa_1$snow), decreasing = TRUE) %>% View()
```

The most commonly observed value is 0 mm because, in the state of NY, it does not 
snow for more than half of the year (i.e. it snows for 4 to 5 months of the year).
Based on this, 0 mm of snow should be the most frequent observations. The second
and third commonly observed values are 25 and 13 mm as. 

## Problem 3.2
```{r}
ny_noaa_2 = ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  select(year, month, tmax, tmin, id) %>%
  group_by(year, id) 


```